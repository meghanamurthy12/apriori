# backpropogation

Backpropogation is a very important concept in neural networks. It is a standard method of training artificial neural networks. This method is used to calculate the gradient of a loss function with respect to all the weights in the network. It follows a gradient descent method exploiting the chain rule of calculus. 

A good explanation of the backpropogation logic can be found [here](http://neuralnetworksanddeeplearning.com/chap2.html)
